{"version":3,"sources":["../../../src/thegamma/tokenizer.fs"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA;;;;;;;;;;;;;;AAkBE;AAAA,WAAG,mBAAH,UACK,cAAL,WACK,kBAAL,WACK,cAAc,KAAd,EAAoB,KAApB,SAHL;AAGoC;;AAGvB;AAAA,YAAC,WAAY,QAAZ,QAAD,WAA2B,WAAY,QAAZ,QAA3B;AAAgD;;AAGhD;AAAA,sBAAY,QAAZ;AAAoB;;AAKjC;AAAA,KAC0C;AAAA;AAAc,KADxD,sBACU,kBAAmB,KAAnB,CADV;;AAEA,8BAAmB,KAAnB;AAAyB;;AAKzB;AAAA,YAAG,+BACE,iBAAkB,SAAlB,YAA8B,iBAAkB,SAAlB,EADhC,QAAH,IAEE,0BAAyB,KAAzB,CAFF,GAIE,oBAAoB,6BAAgB,0BAAhB,EAApB,WAJF;AAI6E;;AAK7E;AAAA,WAAG,6BAAH,GACE,2CADF,GAGQ;AAAA,iCAAW,SAAX;;AAAA;AAAA,qCAUG,0CAAuC,KAAvC,CAVH,yBAII;AAAA,uCAAW,aAAX;;AAAA,qCAIG;AAAA,uCAAoB,oCAApB,SAAqC,KAArC;AAA2C,WAJ9C;AAAA,uCAGI;AAAA,yCAAoB,oCAApB,SAAsC,KAAtC;AAA4C,aAHhD;AAAA,wCACG;AAAA,2CAAoB,oCAApB,SAAsC,KAAtC;AAA4C,eAD/C;AAAA,0CAEG;AAAA,6CAAoB,oCAApB,SAAsC,KAAtC;AAA4C,iBAF/C,MAKC;AAAA,6CAAoB,kDAApB,SAAyC,KAAzC;AAA+C;AALvB;AAAA;AAAA;AAAA,SAAzB,EAJJ,GAWC,oBAAoB,0CAApB,SAAmC,KAAnC,CAXD;AAAA;;AAAA;AAAA,YACM,iCADN,EAEF;AAAA,8CAA4B,oCAA5B,SAA8C,KAA9C;AAAoD,SAFlD,MACM;AAAA;AAAiC;AADlB,OAArB;AAAA;AAAqB;AAAA,KAArB,EAHR;AAcgD;;AAG5C;AAA0C,cAA7B,eAAgB,UAAa;AACpC,oCAAuB,SAAvB;;AACV,eAAc;AAAA,sBAAe,gDAAf;AAA6D;;AAC3E,+BAAoB,mCAApB;AAHO;;AAQP;AAAA,WAAG,6BAAH,GACE,2CADF,GAGQ;AAAA,iCAAW,SAAX;;AAAA,+BACI;AAAA,wDAAuC,KAAvC;AAA6C,OADjD;AAAA,gCAEI;AAAA,2DAAwC,KAAxC;AAA8C,SAFlD,MAGC;AAAA,iDAA+B,KAA/B;AAAqC;AAHjB;AAAA,KAArB,EAHR;AAM8C;;AAG1C;AAAM,oCAAuB,SAAvB;AACA,+BAAoB,SAApB,EAA+B,KAAI,aAAJ,CAA/B;AACA,gBAAG,qCAAH,GAA2B,cAAiB,cAAjB,CAA3B;;AACV,eAAc;AAAA,sBAAe,iDAAf;AAA4D;;AAC1E,+BAAoB,qCAApB;AAJO;;AASP;AAAA,YAAG,+BAAgC,UAAW,SAAX,SAAhC,QAAH,IACE,0BAAyB,KAAzB,CADF,GAGE,oBAAoB,6BAAgB,0BAAhB,EAApB,WAHF;AAG6E;;AAK7E;AAAA,YAAG,+BAAgC,iBAAkB,SAAlB,EAAhC,QAAH,IACE,oCAAkC,KAAlC,CADF,GAEA,CAAK,gCAAgC,QAAhC,YAA+C,UAAW,SAAX,SAA/C,QAAL,IACE,iCAA+B,KAA/B,CADF,GAGM;AAAM;AACV,iCAAoB,mCAAsB,sBAAtB,EAApB;AADO,KAAH,EALN;AAM+D;;AAK/D;AAAA,WAAG,qBAAH,SAGM;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iDAwBI,oBAAmB,iCAAnB,OAxBJ,wBAoCG,wBApCH,yBAmCG,oBAAmB,qBAAnB,OAnCH,wBAqCI,8BArCJ,wBAiBG,oBAAmB,gCAAnB,OAjBH,wBAkBG,oBAAmB,gCAAnB,OAlBH,wBA+BG,oBAAoB,gCAAmB,iCAAnB,EAApB,OA/BH,wBA6BG,oBAAoB,gCAAmB,6BAAnB,EAApB,OA7BH,wBAqBG,oBAAmB,+BAAnB,OArBH,wBA8BG,oBAAoB,gCAAmB,8BAAnB,EAApB,OA9BH,wBAoBG,oBAAmB,6BAAnB,OApBH,wBAgCG,oBAAoB,gCAAmB,+BAAnB,EAApB,OAhCH,wBA4BG,oBAAoB,gCAAmB,iCAAnB,EAApB,OA5BH,wBAmBG,oBAAmB,gCAAnB,OAnBH,wBA2BG,oBAAoB,gCAAmB,oCAAnB,EAApB,OA3BH,wBAsBG,oBAAmB,iCAAnB,OAtBH,wBAuBG,oBAAmB,iCAAnB,OAvBH,GAyCH,kBAAH,GAAiB,wBAAjB,GACK,kBAAL,GAAmB,gCAAnB,GAIA;AAAA,sCAAe,sCAAqC,kBAAmB,KAAnB,CAArC,aAAf;AACA,iDAAoB,yCAApB;AAA0C,qBAD1C,EA9CM;AAAA;;AAAA;AAAA,wBAaK,iCAbL,EAcF;AAAA,iDAAoB,gCAAmB,2CAAnB,EAApB;AAAuE,qBAdrE,MAaK;AAAA;AAA6B;AAbrB,mBAAb;AAAA;AAAa;AAAA,iBAAb;;AAAA;AAAA,sBAWK,iCAXL,EAYF;AAAA,+CAAoB,gCAAmB,wCAAnB,EAApB;AAAoE,mBAZlE,MAWK;AAAA;AAA6B;AAXrB,iBAAb;AAAA;AAAa;AAAA,eAAb;;AAAA;AAAA,oBASK,oCATL,EAUF;AAAA,6CAAoB,sCAApB;AAAgD,iBAV9C,MASK;AAAA;AAAgC;AATxB,eAAb;AAAA;AAAa;AAAA,aAAb;;AAAA;AAAA,kBAOK,mCAPL,EAQF;AAAA,2CAAoB,qCAApB;AAA+C,eAR7C,MAOK;AAAA;AAA+B;AAPvB,aAAb;AAAA;AAAa;AAAA,WAAb;;AAAA;AAAA,gBAKK,kCALL,EAMF;AAAA,yCAAoB,6BAApB;AAAsC,aANpC,MAKK;AAAA;AAA8B;AALtB,WAAb;AAAA;AAAa;AAAA,SAAb;;AAAA;AAAA,cAGK,kCAHL,EAIF;AAAA,uCAAoB,6BAApB;AAAsC,WAJpC,MAGK;AAAA;AAA8B;AAHtB,SAAb;AAAA;AAAa;AAAA,OAAb;;AAAA;AAAA,YACK,iCADL,EAEF;AAAA,qCAAoB,+BAApB;AAAwC,SAFtC,MACK;AAAA;AAA6B;AADrB,OAAb;AAAA;AAAa;AAAA,KAAb,EAHN;AAkD0C;;AAMtC;AACF;AAAA;AAAA;AAEiB,KAFjB;;AAGQ;AAJN,YAKJ,mCALI,EAKmB,wBALnB;AAAG","file":"tokenizer.js","sourceRoot":"c:/tomas/public/thegamma/thegamma-script/out/src/thegamma","sourcesContent":["ï»¿// ------------------------------------------------------------------------------------------------\r\n// Tokenizer for The Gamma script language\r\n// ------------------------------------------------------------------------------------------------\r\n\r\nmodule TheGamma.Tokenizer\r\n\r\nopen TheGamma\r\nopen TheGamma.Parsec\r\n\r\n/// Tokenization context for storing input, errors & parsed tokens\r\ntype Context = \r\n  { Tokens : ResizeArray<Token>\r\n    Errors : ResizeArray<Error<Range>>\r\n    Input : string }\r\n\r\n/// Test whether 's' has 'prefix' at offset 'i'. The\r\n/// parameter 'j' is index inside prefix where we're starting.\r\nlet rec startsWith (s:string) i j (prefix:string) = \r\n  if j = prefix.Length then true\r\n  elif i = s.Length then false\r\n  elif s.[i] <> prefix.[j] then false\r\n  else startsWith s (i+1) (j+1) prefix\r\n\r\n/// Is given character a string?\r\nlet letter c = (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')\r\n\r\n/// Is given character a number?\r\nlet number c = c >= '0' && c <= '9'\r\n\r\n\r\n/// Add newly parsed token to the context & continue tokenizing\r\nlet rec addAndTokenize ctx tok i l =\r\n  { Token = tok\r\n    Range = { Start = i; End = i + l } } |> ctx.Tokens.Add \r\n  tokenizeInput ctx (i + l)\r\n\r\n\r\n/// Tokenize identifier (continue consuming letters & characters)\r\nand tokenizeIdent ctx start l =\r\n  if start + l < ctx.Input.Length && \r\n      (letter ctx.Input.[start+l] || number ctx.Input.[start+l]) then\r\n    tokenizeIdent ctx start (l+1)\r\n  else\r\n    addAndTokenize ctx (TokenKind.Ident(ctx.Input.Substring(start, l))) start l\r\n\r\n\r\n/// Tokenize string (until end of input or closing double-quote)\r\nand tokenizeString ctx acc start l =\r\n  if start + l >= ctx.Input.Length then \r\n    tokenizeStringEnd true ctx acc start l\r\n  else\r\n    match ctx.Input.[start + l] with\r\n    | '\\\\' when start + l + 1 >= ctx.Input.Length ->\r\n        tokenizeStringEnd true ctx ('\\\\'::acc) start (l + 1)\r\n    | '\\\\' ->\r\n        match ctx.Input.[start + l + 1] with\r\n        | 'n' -> tokenizeString ctx ('\\n'::acc) start (l + 2)\r\n        | 't' -> tokenizeString ctx ('\\t'::acc) start (l + 2)\r\n        | '\\\\' -> tokenizeString ctx ('\\\\'::acc) start (l + 2)\r\n        | '\"' -> tokenizeString ctx ('\"'::acc) start (l + 2)\r\n        | c -> tokenizeString ctx (c::'\\\\'::acc) start (l + 2)\r\n    | '\"' -> tokenizeStringEnd false ctx acc start (l + 1)\r\n    | c -> tokenizeString ctx (c::acc) start (l+1)\r\n\r\nand tokenizeStringEnd error ctx acc start l =\r\n  let str = acc |> List.toArray |> Array.rev |> System.String\r\n  let rng = { Start = start; End = start + l }\r\n  if error then ctx.Errors.Add(Errors.Tokenizer.inputEndInsideString rng str) \r\n  addAndTokenize ctx (TokenKind.String(str)) start l\r\n\r\n\r\n/// Tokenize quoted ident (until end of input or closing single-quote)\r\nand tokenizeQuotedIdent ctx start l =\r\n  if start + l >= ctx.Input.Length then \r\n    tokenizeQuotedIdentEnd true ctx start l\r\n  else\r\n    match ctx.Input.[start + l] with\r\n    | '\\n' -> tokenizeQuotedIdentEnd true ctx start (l + 1)\r\n    | '\\'' -> tokenizeQuotedIdentEnd false ctx start (l + 1)\r\n    | c -> tokenizeQuotedIdent ctx start (l + 1)\r\n\r\nand tokenizeQuotedIdentEnd error ctx start l =\r\n  let rng = { Start = start; End = start + l }\r\n  let qid = ctx.Input.Substring(start + 1, l - if error then 1 else 2)\r\n  let qid = if qid.EndsWith(\"\\n\") then qid.Substring(0, qid.Length-1) else qid\r\n  if error then ctx.Errors.Add(Errors.Tokenizer.missingClosingQuote rng qid) \r\n  addAndTokenize ctx (TokenKind.QIdent(qid)) start l\r\n\r\n\r\n/// Tokenize whitespace - consume all spaces available\r\nand tokenizeWhite ctx start l =\r\n  if start + l < ctx.Input.Length && ctx.Input.[start+l] = ' ' then\r\n    tokenizeWhite ctx start (l+1)\r\n  else\r\n    addAndTokenize ctx (TokenKind.White(ctx.Input.Substring(start, l))) start l\r\n\r\n\r\n/// Tokenize number - consume all numbers, or '.' when 'decimal = true'\r\nand tokenizeNumber ctx decimal start l =\r\n  if start + l < ctx.Input.Length && number ctx.Input.[start+l] then\r\n    tokenizeNumber ctx decimal start (l+1)\r\n  elif start + l < ctx.Input.Length && not decimal && ctx.Input.[start+l] = '.' then\r\n    tokenizeNumber ctx true start (l+1)\r\n  else\r\n    let str = ctx.Input.Substring(start, l)\r\n    addAndTokenize ctx (TokenKind.Number(str, float str)) start l\r\n\r\n\r\nand tokenizeInput ctx i = \r\n  // Reached the end of the input\r\n  if i >= ctx.Input.Length then ctx else\r\n\r\n  // Keyword or multi-letter symbol\r\n  match ctx.Input.[i] with\r\n  | '-' when startsWith ctx.Input i 0 \"->\" -> \r\n      addAndTokenize ctx (TokenKind.Arrow) i 2\r\n  | 'f' when startsWith ctx.Input i 0 \"fun\" -> \r\n      addAndTokenize ctx (TokenKind.Fun) i 3\r\n  | 'l' when startsWith ctx.Input i 0 \"let\" -> \r\n      addAndTokenize ctx (TokenKind.Let) i 3\r\n  | 't' when startsWith ctx.Input i 0 \"true\" -> \r\n      addAndTokenize ctx (TokenKind.Boolean true) i 4\r\n  | 'f' when startsWith ctx.Input i 0 \"false\" -> \r\n      addAndTokenize ctx (TokenKind.Boolean false) i 5\r\n  | '<' when startsWith ctx.Input i 0 \"<=\" -> \r\n      addAndTokenize ctx (TokenKind.Operator Operator.LessThanOrEqual) i 2\r\n  | '>' when startsWith ctx.Input i 0 \">=\" -> \r\n      addAndTokenize ctx (TokenKind.Operator Operator.GreaterThanOrEqual) i 2\r\n\r\n  // Single-letter tokens\r\n  | '(' -> addAndTokenize ctx TokenKind.LParen i 1\r\n  | ')' -> addAndTokenize ctx TokenKind.RParen i 1\r\n  | '=' -> addAndTokenize ctx TokenKind.Equals i 1\r\n  | '.' -> addAndTokenize ctx TokenKind.Dot i 1\r\n  | ',' -> addAndTokenize ctx TokenKind.Comma i 1\r\n  | '[' -> addAndTokenize ctx TokenKind.LSquare i 1\r\n  | ']' -> addAndTokenize ctx TokenKind.RSquare i 1\r\n  | '\\n' -> addAndTokenize ctx TokenKind.Newline i 1\r\n\r\n  // Single-letter operators\r\n  | '>' -> addAndTokenize ctx (TokenKind.Operator(Operator.GreaterThan)) i 1\r\n  | '<' -> addAndTokenize ctx (TokenKind.Operator(Operator.LessThan)) i 1\r\n  | '+' -> addAndTokenize ctx (TokenKind.Operator(Operator.Plus)) i 1\r\n  | '-' -> addAndTokenize ctx (TokenKind.Operator(Operator.Minus)) i 1\r\n  | '*' -> addAndTokenize ctx (TokenKind.Operator(Operator.Multiply)) i 1\r\n  | '/' -> addAndTokenize ctx (TokenKind.Operator(Operator.Divide)) i 1\r\n  \r\n  // Symbols that start something (string, whitespace, quoted ident)\r\n  | '\"' -> tokenizeString ctx [] i 1\r\n  | ' ' -> tokenizeWhite ctx i 1\r\n  | '\\'' -> tokenizeQuotedIdent ctx i 1\r\n  | c ->\r\n  \r\n  // Letter starts identifer, number starts number\r\n  if letter c then tokenizeIdent ctx i 1\r\n  elif number c then tokenizeNumber ctx false i 1\r\n  else \r\n\r\n  // Otherwise report an error & skip one character\r\n  ctx.Errors.Add(Errors.Tokenizer.unexpectedCharacter { Start = i; End = i + 1 } c)\r\n  addAndTokenize ctx (TokenKind.Error c) i 1\r\n\r\n\r\n/// Tokenize the given input. Consumes all input characters and returns\r\n/// list of parsed tokens together with an array of tokenization errors.\r\nlet tokenize input = \r\n  let ctx = \r\n    { Errors = new ResizeArray<_>()\r\n      Tokens = new ResizeArray<_>()\r\n      Input = input }\r\n  let ctx = tokenizeInput ctx 0\r\n  List.ofSeq ctx.Tokens, ctx.Errors.ToArray()"]}